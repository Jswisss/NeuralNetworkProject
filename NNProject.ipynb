{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RodaZKDoHcLb",
        "outputId": "33fdb4e3-2c28-4019-ea11-0179c33b2b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108941\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "import string\n",
        "#data_path = \"Downloads/raw.tar/raw/raw_text.txt\"\n",
        "data_path = \"Downloads/raw.tar/raw/jpn.txt\"\n",
        "with open(data_path, 'r', encoding='utf-8') as j:\n",
        "  lines = j.read()\n",
        "\n",
        "#Organizes our data so that we can start training the model\n",
        "#since our data before this process is just a somewhat structure text file.\n",
        "def to_lines(text):\n",
        "  sents = text.strip().split('\\n')\n",
        "  sents = [i.split('\\t') for i in sents]\n",
        "  return sents\n",
        "\n",
        "data = to_lines(lines)\n",
        "# We should be see 167.130 french samples to be used for our model\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:10])\n",
        "array_data = array(data)\n",
        "#array_data = np.delete(array_data,2,1)\n",
        "print(array_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxa14q75WjTU",
        "outputId": "abefd1f9-f2cd-4cc5-93e6-81a0500f7f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Go.', '行け。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7421985 (Ninja)'], ['Go.', '行きなさい。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7421986 (Ninja)'], ['Hi.', 'こんにちは。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #373351 (tommy_san)'], ['Hi.', 'もしもし。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #385517 (mookeee)'], ['Hi.', 'やっほー。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3480285 (arnab)'], ['Hi.', 'こんにちは！', 'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3480287 (arnab)'], ['Run.', '走れ。', 'CC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #5955868 (tatoebane)'], ['Run.', '走って！', 'CC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #5955869 (tatoebane)'], ['Who?', '誰？', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #638666 (arihato)'], ['Wow!', 'すごい！', 'CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #214733 (arihato)']]\n",
            "(108941, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing our data\n",
        "#Removing punctuation\n",
        "array_data[:,0] = [s.translate(str.maketrans('','',string.punctuation)) for s in array_data[:,0]]\n",
        "array_data[:,1] = [s.translate(str.maketrans('','',string.punctuation)) for s in array_data[:,1]]\n",
        "#Making all characters lower case\n",
        "for i in range(len(array_data)):\n",
        "  array_data[i,0] = array_data[i,0].lower()\n",
        "  array_data[i,1] = array_data[i,1].lower()\n",
        "\n",
        "smaller_data_set = array_data"
      ],
      "metadata": {
        "id": "s01muAcWWmcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#Tokenizer function to create tokenizer for our data sample sets\n",
        "def tokenization(lines):\n",
        "  tokenizer =Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "medium_size_set = array_data\n",
        "#create english tokenizer\n",
        "eng_data_tokenized = tokenization(medium_size_set[:,0])\n",
        "eng_vocab_size = len(eng_data_tokenized.word_index) + 1\n",
        "#create japanese & French tokenizer\n",
        "sample_data_tokenized = tokenization(medium_size_set[:,1])\n",
        "sample_vocab_size = len(sample_data_tokenized.word_index) + 1\n",
        "\n",
        "#Created a varaible that sets the max word length in a sentence\n",
        "eng_length = 47\n",
        "sample_length = 54\n",
        "print(eng_vocab_size, sample_vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-seTZJRc4LO",
        "outputId": "e0e9bf01-c8f6-4cad-b48e-faa0dd2d9207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12159 89247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#encode and pad sequences, padding to a maxium sentence length as mention earlier\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "  #integer encode sequences\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  #pad sequences with 0 values\n",
        "  seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "  return seq"
      ],
      "metadata": {
        "id": "K91AZsgDfuMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Splitting data into train and test set to be used by our model\n",
        "train_set, test_set = train_test_split(smaller_data_set, test_size=0.02, random_state=42)\n",
        "\n",
        "train_sample = encode_sequences(sample_data_tokenized, sample_length, train_set[:,1])\n",
        "train_english = encode_sequences(eng_data_tokenized, eng_length, train_set[:,0])\n",
        "\n",
        "test_sample = encode_sequences(japan_data_tokenized, sample_length, test_set[:,1])\n",
        "test_english = encode_sequences(eng_data_tokenized, eng_length, test_set[:,0])"
      ],
      "metadata": {
        "id": "_GQbTCGZiwjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, RepeatVector, GRU, Bidirectional, TimeDistributed, Attention\n",
        "#building our NMT model to be used to predict our english translation of our sample text\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(in_vocab, units,  mask_zero=True))\n",
        "  model.add(Bidirectional(LSTM(units, return_sequences=False)))\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "  model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
        "  model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n",
        "  return model"
      ],
      "metadata": {
        "id": "69aLSR-ClKF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "model = define_model(sample_vocab_size, eng_vocab_size, sample_length, eng_length, 512)\n",
        "model.build((None,10))\n",
        "print(model.summary())\n",
        "adam = optimizers.Adam(learning_rate=.001)\n",
        "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "huBCcTvi30IS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "771e6545-64b0-4976-8dc9-56a4296e6b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │      \u001b[38;5;34m45,694,464\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m4,198,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector_2 (\u001b[38;5;33mRepeatVector\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m1024\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m1024\u001b[0m)            │       \u001b[38;5;34m6,295,552\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_2 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m12159\u001b[0m)           │      \u001b[38;5;34m12,462,975\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">45,694,464</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,295,552</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12159</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,462,975</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m68,651,391\u001b[0m (261.88 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,651,391</span> (261.88 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m68,651,391\u001b[0m (261.88 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,651,391</span> (261.88 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training our model\n",
        "fit_model = model.fit(train_sample, train_english.reshape(train_english.shape[0], train_english.shape[1], 1), epochs=10, batch_size=100, validation_split = 0.20)\n"
      ],
      "metadata": {
        "id": "aGAfjTV_54N1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f3e8c1-8f49-4a9c-d9dd-3a151e3ae9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5920s\u001b[0m 7s/step - accuracy: 0.8778 - loss: 1.2713 - val_accuracy: 0.8886 - val_loss: 0.7983\n",
            "Epoch 2/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5831s\u001b[0m 7s/step - accuracy: 0.8897 - loss: 0.7617 - val_accuracy: 0.8879 - val_loss: 0.7943\n",
            "Epoch 3/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5874s\u001b[0m 7s/step - accuracy: 0.8923 - loss: 0.7077 - val_accuracy: 0.8813 - val_loss: 0.8161\n",
            "Epoch 4/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5723s\u001b[0m 7s/step - accuracy: 0.8967 - loss: 0.6497 - val_accuracy: 0.8807 - val_loss: 0.8212\n",
            "Epoch 5/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5717s\u001b[0m 7s/step - accuracy: 0.9034 - loss: 0.5733 - val_accuracy: 0.8826 - val_loss: 0.8481\n",
            "Epoch 6/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5640s\u001b[0m 7s/step - accuracy: 0.9115 - loss: 0.4920 - val_accuracy: 0.8816 - val_loss: 0.8877\n",
            "Epoch 7/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5708s\u001b[0m 7s/step - accuracy: 0.9193 - loss: 0.4203 - val_accuracy: 0.8793 - val_loss: 0.9282\n",
            "Epoch 8/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5601s\u001b[0m 7s/step - accuracy: 0.9263 - loss: 0.3608 - val_accuracy: 0.8776 - val_loss: 0.9498\n",
            "Epoch 9/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5812s\u001b[0m 7s/step - accuracy: 0.9337 - loss: 0.3055 - val_accuracy: 0.8771 - val_loss: 0.9883\n",
            "Epoch 10/10\n",
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6027s\u001b[0m 7s/step - accuracy: 0.9408 - loss: 0.2603 - val_accuracy: 0.8760 - val_loss: 1.0331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting our translation\n",
        "training_sample = test_sample[:2000]\n",
        "preds = model.predict(training_sample.reshape((training_sample.shape[0], training_sample.shape[1])))\n"
      ],
      "metadata": {
        "id": "8FD0Uwxv6aZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea20740-f30c-4546-97f5-43cfa7b8569a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 642ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#these predictions are sequences of integers. We need to convert these integers to their corresponding words\n",
        "import numpy as np\n",
        "classes_x=np.argmax(preds,axis=-1)\n",
        "print(classes_x[0], test_english[0])\n",
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == n[0]:\n",
        "      return word\n",
        "  return None"
      ],
      "metadata": {
        "id": "McaTP7rC6959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bedd55-69c5-4ea2-c08c-61a65457e1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 11  54  18 711   9 293 900   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0] [23 70  3 35  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert opur prediction results into our english sentences\n",
        "preds_text = []\n",
        "for i in classes_x:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    word = [i[j]]\n",
        "    t = get_word(word, eng_data_tokenized)\n",
        "    if j>0:\n",
        "      previous_word = [i[j-1]]\n",
        "      if (t == get_word(previous_word, eng_data_tokenized)) or (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "    else:\n",
        "      if(t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "  preds_text.append(' '.join(temp))"
      ],
      "metadata": {
        "id": "904Qq-oU7JJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample_english = test_set[:2000]\n",
        "pred_df = pd.DataFrame({'actual': test_sample_english[:,0], 'predicted': preds_text})\n",
        "#Sample our results\n",
        "pred_df.sample(15)"
      ],
      "metadata": {
        "id": "xTCdGr7_8mBc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "c25fa3c3-b7d9-40be-fb2b-2ea36610503d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              actual  \\\n",
              "1179                      i rang the bell and waited   \n",
              "435                           lets try one more time   \n",
              "1672                     does anybody have a kleenex   \n",
              "13                              i dozed off in class   \n",
              "1590                       we stayed at a farm house   \n",
              "1673                   i get up early in the morning   \n",
              "1317                                im working on it   \n",
              "1826                               i lost my glasses   \n",
              "1268                       ive forgotten your number   \n",
              "962               mary was wearing a navy blue skirt   \n",
              "1525                would you mind doing the laundry   \n",
              "1674                            where is your father   \n",
              "997             he can speak thai as well as english   \n",
              "1910  the hotel is within easy access of the station   \n",
              "388                        ill take back what i said   \n",
              "\n",
              "                                              predicted  \n",
              "1179  he is me of this same he                      ...  \n",
              "435   lets try again                                ...  \n",
              "1672  does anyone have a tissue                     ...  \n",
              "13    he happened about this  and gets              ...  \n",
              "1590  there is clear of this same heart             ...  \n",
              "1673  im an  riser                                  ...  \n",
              "1317  tom is me of same he                          ...  \n",
              "1826  ive lost my glasses                           ...  \n",
              "1268  i dialed your telephone number                ...  \n",
              "962   this was the matter of heart                  ...  \n",
              "1525  she were came from it heart                   ...  \n",
              "1674  he took out the best of heart away            ...  \n",
              "997   it were me from all same yesterday            ...  \n",
              "1910  he is me of this best heart                   ...  \n",
              "388   were  me of the same he                       ...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>i rang the bell and waited</td>\n",
              "      <td>he is me of this same he                      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>lets try one more time</td>\n",
              "      <td>lets try again                                ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1672</th>\n",
              "      <td>does anybody have a kleenex</td>\n",
              "      <td>does anyone have a tissue                     ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>i dozed off in class</td>\n",
              "      <td>he happened about this  and gets              ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1590</th>\n",
              "      <td>we stayed at a farm house</td>\n",
              "      <td>there is clear of this same heart             ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>i get up early in the morning</td>\n",
              "      <td>im an  riser                                  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>im working on it</td>\n",
              "      <td>tom is me of same he                          ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1826</th>\n",
              "      <td>i lost my glasses</td>\n",
              "      <td>ive lost my glasses                           ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>ive forgotten your number</td>\n",
              "      <td>i dialed your telephone number                ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>mary was wearing a navy blue skirt</td>\n",
              "      <td>this was the matter of heart                  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1525</th>\n",
              "      <td>would you mind doing the laundry</td>\n",
              "      <td>she were came from it heart                   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1674</th>\n",
              "      <td>where is your father</td>\n",
              "      <td>he took out the best of heart away            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>he can speak thai as well as english</td>\n",
              "      <td>it were me from all same yesterday            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1910</th>\n",
              "      <td>the hotel is within easy access of the station</td>\n",
              "      <td>he is me of this best heart                   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>ill take back what i said</td>\n",
              "      <td>were  me of the same he                       ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU save file to go to our BLEU score script\n",
        "import numpy\n",
        "df = pd.DataFrame(preds_text)\n",
        "df.to_csv(\"Downloads/raw.tar/raw/exppred512jpn.csv\")\n",
        "df = pd.DataFrame(test_sample_english)\n",
        "df.to_csv(\"Downloads/raw.tar/raw/exptestsample512jpn.csv\")"
      ],
      "metadata": {
        "id": "sBCSnUihvyhU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}